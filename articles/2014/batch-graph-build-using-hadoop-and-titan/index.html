<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        
        <title>How Project Rhino leverages Hadoop to build a graph of the music world - Stephen Holiday</title>
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <link href="http://feeds.feedburner.com/StephenHoliday" rel="alternate" title="" type="application/atom+xml"/>
        <link href="//netdna.bootstrapcdn.com/bootswatch/3.1.1/yeti/bootstrap.min.css" rel="stylesheet">
        <link rel="stylesheet" href="/media/css/pygments.css">
        <link href='//fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
        <!--<style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>-->
        <link rel="stylesheet" href="/media/css/main.css">
        <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/favicon.ico" type="image/x-icon">
    </head>
    <body>
        <div class="navbar navbar-default navbar-fixed-top">
          <div class="container">
            <div class="navbar-header">
              <a href="/" class="navbar-brand">Stephen Holiday</a>
              <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="navbar-collapse collapse" id="navbar-main">
              <ul class="nav navbar-nav">
                <li ><a href="/articles">Articles</a></li>
                <li ><a href="/projects">Projects</a></li>
                <li ><a href="/notes">Notes</a></li>
                <li ><a href="/travel">Travel</a></li>
                <li><a href="/resume">Resume</a></li>
                <li ><a href="/contact">Contact</a></li>
              </ul>

            </div>
          </div>
        </div>

        <div id="container-main" class="container clearfix">
          <div style="margin:20px">
            
<div id="content-header"><h1>How Project Rhino leverages Hadoop to build a graph of the music world</h1></div>



<div id="post">
<p><img src="/media/img/rhino-graph-build/stephen-symposium.jpg" style="margin:10px;float:left" /></p>

<p>This last week <a href="http://tony-dong.com">Tony Dong</a>, <a href="http://www.linkedin.com/pub/omid-mortazavi/41/a97/5b0">Omid Mortazavi</a>, <a href="http://www.linkedin.com/in/vineetnayak">Vineet Nayak</a>
and I presented <a href="http://tryrhino.com">Project Rhino</a> at this years
<a href="https://uwaterloo.ca/engineering/events/electrical-and-computer-engineerings-capstone-design">ECE Capstone Design Symposium</a> at the University of Waterloo.</p>

<p>Project Rhino is a music search engine that allows you to ask questions about
the music world in plain English. Check out a <a href="http://vimeo.com/89340593">demo video here</a>.</p>

<p>We noticed there is a ton of data available about the music world but that they
were disconnected. There was no easy way to explore the relationships in the
music world.</p>

<p>This project allows a user to express an English query, that is then transformed
into a traversal over the music data we have collected. All of the data was
retrieved from freely available Creative Commons sources. However,
integrating the data from disparate sources is non-trivial.</p>

<p><br /></p>

<p>Here are some of the questions Project Rhino can answer:</p>

<ul>
<li>Find artists similar to "The Beatles" and played in "Waterloo, Ontario"</li>
<li>Find artists from Canada and similar to "Vampire Weekend"</li>
<li>Find songs by artists from Australia and similar to artists from Germany</li>
<li>Show venues where artists born in 1970 and from Japan played</li>
</ul>

<p>Aside from the complexities of data integration, the sheer volume of data (100+
GB of compressed JSON) makes this project challenging.</p>

<p>In this post I focus on one aspect of the ETL pipeline I developed for Project
Rhino, the construction and insertion of the graph into our graph database.</p>

<p><img src="/media/img/rhino-graph-build/titan-logo.png" style="margin:10px;float:right" /></p>

<h3 id="the-graph-database">The Graph Database</h3>

<p>We used <a href="http://thinkaurelius.github.io/titan/">Titan</a> on top of <a href="https://cassandra.apache.org/">Cassandra</a> to store our graph. It’s
a pretty cool project and worth checking out. They provide a nice graph
abstraction and excellent graph traversal language called <a href="https://github.com/tinkerpop/gremlin/wiki">Gremlin</a>.</p>

<p>When we first came up with the idea for this project at <a href="http://www.mortys.com/">Morty’s Pub</a>,
we were going to build our own distributed graph database from scratch. Titan
saved us a lot of pain.</p>

<p>The people who produce Titan have a batch graph engine built on top of Hadoop
called <a href="http://thinkaurelius.github.io/faunus/">Faunus</a>. It’s pretty cool but we didn’t end up using it for some
of the reasons I’ll talk about later.</p>

<h2 id="rhinos-batch-graph-build">Rhino’s Batch Graph Build</h2>

<p>The final and most intensive stage of the pipeline is graph construction. This
is when the data is combined to create the graph. The graph is made up of two
components: nodes with properties (ex. an artist) and edges between nodes with
properties (ex. <code>writtenBy</code>). This stage outputs a list of graph nodes and then
a list of graph edges. This process is managed by a series of Hadoop MapReduce
jobs.</p>

<h3 id="hadoop-vertex-format">Hadoop Vertex Format</h3>

<p>The first step is to transform the intermediate form tables into nodes and
edges. Since Hadoop requires that data be serialized between steps, I needed a
way to represent the graph on disk. I chose to use a Thrift Struct.</p>

<p>Here’s the Thrift definition for a vertex in the ETL pipeline:</p>

<pre><code>struct TVertex {
  1: optional i64 rhinoId,
  2: optional i64 titanId,
  3: optional map&lt;string, Item&gt; properties,
  4: optional list&lt;TEdge&gt; outEdges
}
</code></pre>

<p>The <code>rhinoId</code> is the ID used in the intermediate tables. This is the ID we
assigned as part of the ETL process. <code>titanId</code> refers to the identifier Titan
generates for the vertex. The distinction is discussed in further detail in the
Design Decisions section below.</p>

<p>The outEdges field is a list of graph edges with the current vertex as its
source (or left hand of the arrow).</p>

<p>The <code>properties</code> field is a map of string keys to typed value as described here:</p>

<pre><code>union Item {
  1: i16 short_value;
  2: i32 int_value;
  3: i64 long_value;
  4: double double_value;
  5: string string_value;
  6: binary bytes_value;
}
</code></pre>

<p>Here is the Thrift definition for an edge in the ETL pipeline.</p>

<pre><code>struct TEdge {
    1: optional i64 leftRhinoId,
    2: optional i64 leftTitanId,
    3: optional i64 rightRhinoId,
    4: optional i64 rightTitanId,
    5: optional string label,
    6: optional map&lt;string, Item&gt; properties
}
</code></pre>

<h3 id="mapreduce-jobs">MapReduce Jobs</h3>

<h4 id="vertex-jobs">Vertex Jobs</h4>

<p>The first set of jobs is responsible for converting the intermediate tables into
Thrift. Here is an example of the conversions of two tables into its Thrift
form.</p>

<div style="text-align:center;width:100%">
<img src="/media/img/rhino-graph-build/vertex-jobs.png" />
</div>

<p>Now all of the tables follow the same structure and we can treat all vertices
the same way!</p>

<h4 id="edge-jobs">Edge Jobs</h4>

<p>The next set of jobs transforms intermediate edge tables into edges. In
practice, vertex and edge conversion jobs can (and are) run simultaneously.</p>

<div style="text-align:center;width:100%">
<img src="/media/img/rhino-graph-build/edge-jobs.png" />
</div>

<p>Note that instead of producing <code>TEdge</code> structures, TVertex structures are
produced. To facilitate bulk loading, edges are treated like vertices that have
no properties. This allows the next stage to treat records from vertex jobs and
edge jobs identically.</p>

<h4 id="graph-combine-job">Graph Combine Job</h4>

<p>The next stage is a single job that combines vertices and edges by joining them
on <code>titanVertexId</code>. As pictured, both vertex and edge records are combined
together so that each vertex appears only once in the output.</p>

<div style="text-align:center;width:100%">
<img src="/media/img/rhino-graph-build/graph-combine-job.png" />
</div>

<h4 id="vertex-insert-job">Vertex Insert Job</h4>

<p>The Vertex Insert Job is the first job that actually inserts data into the
graph. There are two important phases of this job, the mapper and the reducer.
The mapper, as shown below writes each vertex to Titan.</p>

<p>When it writes the vertex to Titan, it receives an opaque ID that Titan has
assigned to the vertex. The mapping between <code>rhinoID</code> and <code>titanId</code> is written
out so that it can be used in the reduce phase to be matched with all possible
incoming edges.</p>

<p>Next, all of the outgoing edges are written out, grouping by target instead of
source vertex. The key is the target (the right hand vertex) <code>rhinoID</code> and the
value now includes the source (the left hand vertex) <code>titanId</code>.</p>

<div style="text-align:center;width:100%">
<img src="/media/img/rhino-graph-build/vertex-insert-mapper.png" />
</div>

<p>In the reduce phase, all of the records with the same <code>rhinoId</code> (including the
record showing the mapping to Titan ID) are processed at the same time. The
<code>titanId</code> for the given Rhino ID will always appear first due to the use of a
custom sort function not described here.</p>

<p>For each subsequent record (now all edges), the <code>rightTitanId</code> is added to the
edge and written out. At this point the edges could be written directly to Titan.
I’ll talk about why we don’t later on.</p>

<div style="text-align:center;width:100%">
<img src="/media/img/rhino-graph-build/vertex-insert-reducer.png" />
</div>

<h4 id="edge-insert-job">Edge Insert Job</h4>

<p>The edge insert job is fairly straightforward. The mapper reads in the edges one
at a time and adds an edge between the source and target vertices. This is a map
only job and there are no outputs other than the insertions into the graph.</p>

<h3 id="design-decisions">Design Decisions</h3>

<h4 id="custom-batch-framework">Custom Batch Framework</h4>

<p>I initially tried to use Titan’s own batch framework. Two problems arose. First,
there was limited room for insert optimizations. Second, it was not compatible
with the newer version of Hadoop that we were using.</p>

<p>This also allowed me to devise a custom serialization schema. I chose Thrift
because of my previous experience with it and it was already being used for RPC.
Alternatively we could have used something like Protocol Buffers or Avro.</p>

<p>The use of an <code>Item</code> union over a plain byte string allowed for property values
to be stored compactly on disk while maintain type-safety throughout the graph
build process.</p>

<h4 id="vertex-ids">Vertex IDs</h4>

<p>One of the most (surprisingly) challenging issues I faced was identifying
vertices. When a vertex is created in Titan, a vertex ID is generated by Titan
through an opaque process. That is, the user does not know ahead of time what
that ID could be. In the intermediate tables each vertex is given a unique ID
referred to as the Rhino ID, however it is not known ahead of time what
the Titan ID will be.</p>

<p>This becomes an issue when as part of a distributed insert, an edge is to be
inserted between two vertices. While the edge contains the source and target
(left and right hand vertices), looking up each vertex’s Titan ID is not
straightforward.</p>

<p>I considered storing the Rhino ID as an indexed property on the vertex, however
that requires extra storage that would be wasted after insertion was complete.
More importantly, looking up a Rhino ID in the distributed graph could require a
network hop to the node that has the Titan ID in question. Instead I opted to
use the information already available and perform the insertion in two stages as
described.</p>

<h4 id="separate-vertex-and-edge-insert-jobs">Separate Vertex and Edge Insert Jobs</h4>

<p>While the vertices could be inserted in the map phase and the edges in the
reduce phase, I chose to separate them. Initially it was a single job, however
it was difficult to reasons about the jobs and their performance.</p>

<p>Additionally, I needed finer granularity over the transaction size. Since the
two inserts are separated, I can tune the edge insert job to do fewer inserts in
a transaction because edge inserts require more memory as well as I/O bandwidth.
This is reasonable given that inserting edges requires random lookups to obtain
the source and target vertices. In the future, some optimizations can be made to
partition the edge inserts so that they are more cache friendly.</p>

<h4 id="transactions">Transactions</h4>

<p>Each map task is its own transaction. This means that if a task fails, the
segment of data it was working on will be replayed after the original failed
transaction is rolled back. A Hadoop node can fail and the insert will recover
and rerun the transaction on another node.</p>

<p>However the initial size of a map task was quite large, on the order of hundreds
of megabytes of compressed data. Since the Titan client keeps the transaction in
memory until it is committed, this was causing Out of Memory errors.</p>

<p>I set the maximum task size to be about 10 MB of compressed data for inserts.
This did increase the scheduling overhead and the overall insertion time.
However there were no longer Out of Memory errors.</p>

<h3 id="conclusion">Conclusion</h3>
<p>I’ve put the code up on github so <a href="https://github.com/sholiday/rhino-titan-hadoop">check it out</a>!</p>

<p>Thanks to <a href="http://tony-dong.com">Tony Dong</a> for editing this post.</p>


</div>

<div id="related">
  <h3><em>Related Posts</em></h2>
  	<ul class="list-group">
    
      <li class="list-group-item"><a href="/articles/2014/that-first-co-op-job"><strong>That First Co-op Job</strong></a></li>
    
      <li class="list-group-item"><a href="/articles/2014/backup-strategy"><strong>My Backup Strategy</strong></a></li>
    
      <li class="list-group-item"><a href="/articles/2013/smtp-email-relay-for-gmail-oozie-hadoop"><strong>SMTP Email Relay for GMail (TLS) with Oozie Using Postfix</strong></a></li>
    
    </ul>
</div>

            <hr>

            
          </div>
          <footer>
              <div class="row">
                <div class="col-lg-12">

                  <ul class="list-unstyled">
                    <li class="pull-right"><a href="#top">Back to top</a></li>
                    <li><a href="/">Home</a></li>
                    <li><a href="/articles">Articles</a></li>
                    <li><a href="/projects">Projects</a></li>
                    <li><a href="/notes">Notes</a></li>
                    <li><a href="/travel">Travel</a></li>
                    <li><a href="/resume">Resume</a></li>
                    <li><a href="/contact">Contact</a></li>
                    <li><a href="http://feeds.feedburner.com/StephenHoliday" rel="alternate" type="application/rss+xml">RSS</a></li>
                    <li><a href="http://ca.linkedin.com/in/stephenholiday"><img src="http://s.c.lnkd.licdn.com/scds/common/u/img/webpromo/btn_profile_greytxt_80x15.png" width="80" height="15" border="0" alt="View Stephen Holiday's profile on LinkedIn"></a></li>
                  </ul>
                  <br />
                  <br>
                  <p>&copy; Stephen Holiday - stephen@stephenholiday.com</p>
                </div>
                
              </div>

            </footer>
        </div> <!-- /container -->

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>

        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-17445820-1']);
          _gaq.push(['_setDomainName', '.stephenholiday.com']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();

        </script>
    </body>
</html>