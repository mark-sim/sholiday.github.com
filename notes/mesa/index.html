<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        
        <title>Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing - Stephen Holiday</title>
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <link href="http://feeds.feedburner.com/StephenHoliday" rel="alternate" title="" type="application/atom+xml"/>
        <link href="//netdna.bootstrapcdn.com/bootswatch/3.1.1/yeti/bootstrap.min.css" rel="stylesheet">
        <link rel="stylesheet" href="/media/css/pygments.css">
        <link href='//fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
        <!--<style>
            body {
                padding-top: 60px;
                padding-bottom: 40px;
            }
        </style>-->
        <link rel="stylesheet" href="/media/css/main.css">
        <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/favicon.ico" type="image/x-icon">
    </head>
    <body>
        <div class="navbar navbar-default navbar-fixed-top">
          <div class="container">
            <div class="navbar-header">
              <a href="/" class="navbar-brand">Stephen Holiday</a>
              <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
            </div>
            <div class="navbar-collapse collapse" id="navbar-main">
              <ul class="nav navbar-nav">
                <li ><a href="/articles">Articles</a></li>
                <li ><a href="/projects">Projects</a></li>
                <li ><a href="/notes">Notes</a></li>
                <li ><a href="/travel">Travel</a></li>
                <li><a href="/resume">Resume</a></li>
                <li ><a href="/contact">Contact</a></li>
              </ul>
              <form class="navbar-form navbar-right" role="search" method="get" id="search" action="http://duckduckgo.com/">
                <div class="form-group">
                   
                  <input type="text" class="form-control" name="q" maxlength="255" placeholder="Search">
                  <input type="hidden" name="sites"value="stephenholiday.com"/>
                  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
                </div>
              </form>
            </div>

          </div>

        </div>

        <div id="container-main" class="container clearfix">
          <div style="margin:20px">
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<div id="content-header">
<h1>Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing <small>- Google, 2014</small></h1>
</div>

<div id="post">
<ul>
  <li>[<a href="http://research.google.com/pubs/archive/42851.pdf">Paper</a>]
[<a href="http://notes.stephenholiday.com/Mesa.pdf">Mirror</a>]</li>
</ul>

<p>Like all my notes in this series, I’ve only included portions I’ve found
interesting.</p>

<ul>
  <li>Notable Features:
    <ul>
      <li>Atomic updates across tables (including materialized views and indexes)</li>
      <li>Strong consistency guarantees (repeatable queries at a version number)</li>
      <li>Geo-replicated (Using redundant work)
        <ul>
          <li>Data is replicated asynchronously, metadata synchronously</li>
          <li>I think this ideas is really interesting</li>
          <li>They use Paxos for metadata but don’t mention PaxosDB. Perhaps they used
something else.</li>
          <li>Separating the data from metadata allows replicas to know if they are out
of data quickly</li>
        </ul>
      </li>
      <li>Uses batched updates (near real-time)
        <ul>
          <li>They tune it for consistency on the order of minutes</li>
        </ul>
      </li>
      <li>Availability
        <ul>
          <li>The authors claim that there is no downtime during a datacenter event</li>
          <li>They cheat by providing a lot of redundancy</li>
          <li>You still need a majority in paxos to have an update. They should have
specified that they allow consistent reads at an old (committed) version</li>
        </ul>
      </li>
      <li>They aim for 99th percentile latencies to be “in the hundreds of
milliseconds” for point queries
        <ul>
          <li>This seems slow if they wish to serve web clients</li>
        </ul>
      </li>
      <li>Schema Changes
        <ul>
          <li>Like <a href="/notes/spanner/">F1</a>, they support online schema changes</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>Why not datastore x?</p>

    <p>I was thinking this question when I first heard of this paper.</p>

    <ul>
      <li><a href="/notes/bigtable/">Bigtable</a> does not support multi-row (let alone
multi-table) transactions</li>
      <li>Megastore, <a href="/notes/spanner/">Spanner</a>, and <a href="/notes/f1/">F1</a> could not stand
up to the update throughput requirements
        <ul>
          <li>Hence the batching</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Data Model
    <ul>
      <li>They recognize that a single fact may be part of many other aggregations 
(materialized views)</li>
      <li>Each table defines a key space, <em>K</em>, and value space, <em>V</em>.</li>
      <li>
        <p>The keys and values can be made up of multiple columns</p>

        <p>For example: <code>(Date, Publisher, Country) -&gt; (ClickCount, Cost)</code></p>
      </li>
      <li>Each value column had a defined <strong>aggregation function</strong>
        <ul>
          <li>Must be associative, often commutative</li>
          <li>A sum for example</li>
        </ul>
      </li>
      <li>Each table can have multiple indexes. These are just total orderings of the
keys</li>
      <li>Materialized views must use the same aggregation functions</li>
    </ul>
  </li>
  <li>Updates
    <ul>
      <li>Updates are batched together by an external system
        <ul>
          <li>They don’t mention it, but it seems a perfect job for
<a href="/notes/millwheel/">MillWheel</a></li>
          <li>They created a library to construct and verify well-formed batches</li>
          <li>These batches are assigned a sequential version number via Paxos</li>
          <li>The batch contains a single aggregated value for each updated key</li>
        </ul>
      </li>
      <li>Each instance of Mesa applies update batches in order
        <ul>
          <li>By maintaining order they do not require commutative aggregations</li>
          <li>Especially useful when they wish to revert a change by adding the negative</li>
          <li>“A negative fact can never be incorporated before it’s positive
counterpart”</li>
        </ul>
      </li>
      <li>Mesa will compute the updated to views and indexes (in each DC)</li>
    </ul>
  </li>
  <li>Version Management
    <ul>
      <li>The ordered collection of updates is enough to serve queries, but is not
efficient</li>
      <li>They define a delta as the aggregations of versions the versions <code>[Vx, Vy]</code>
(Where <code>x &lt; y</code>)</li>
      <li>With a delta, they can compress many batches together. They keep the
individual versions for a period of time to serve queries at a particular
version</li>
      <li>Deltas can be aggregated together as well (as long as they are applied in
order)
        <ul>
          <li>I note that we’ve created a LSM with aggregation functions</li>
          <li>Deltas are stored sorted by key (linear merge)</li>
        </ul>
      </li>
      <li>They periodically compact old versions into a <em>base</em> <code>[V0, B]</code>
        <ul>
          <li>They give the example of 24 hours</li>
          <li>Then they delete old deltas and versions covered by the base</li>
        </ul>
      </li>
      <li>
        <p>The <em>delta compaction policy</em> determines how many deltas and individual
versions to keep around and when to create new deltas to subsume smaller
deltas</p>

        <p>Specifically:</p>
        <ul>
          <li>What deltas must be created before a version can be queried
            <ul>
              <li>For performance, I assume</li>
            </ul>
          </li>
          <li>When deltas should be created outside of the update path</li>
          <li>When deltas can be deleted</li>
        </ul>
      </li>
      <li>The authors note that even though a new base may have been generated,
new deltas relative to the new base must also be created
        <ul>
          <li>Otherwise performance would be abysmal</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Physical Data
    <ul>
      <li>They use an <a href="/notes/rcfile/">RCFile</a> style file format
        <ul>
          <li>(Partition the data into row-groups and then store data column wise for
compression and I/O efficiency)</li>
        </ul>
      </li>
      <li>Indexes are also materialized, they store a complete set of the data
        <ul>
          <li>I’m surprised they didn’t opt to make the index a materialized view
instead of a special case. I’d be interested to hear the reasoning behind
this.</li>
          <li>To be fair, it’s not a special case, each data file belongs to a specific
index. There’s no notion of the main data order.</li>
        </ul>
      </li>
      <li>Each data file has an index file that stores fixed size prefixes of the
first row key for each row group along with the offset</li>
    </ul>
  </li>
  <li>Single DC Architecture
    <ul>
      <li>Each DC instance can scale independently and each subsystem can scale
independently</li>
      <li><strong>Update and Maintenance Subsystem</strong>
        <ul>
          <li>Uses a controller (master) / worker architecture</li>
          <li>Controller maintains the metadata in BigTable
            <ul>
              <li>It’s the only writer of the metadata in BigTable</li>
            </ul>
          </li>
          <li>Controller listens to a metadata feed of table updates</li>
          <li>Controller schedules the work to be done in different queues
            <ul>
              <li>There is a pool of workers for each job type
                <ul>
                  <li>Allows them to scale independently</li>
                  <li>They use their cluster scheduling system to dynamically adjust the
number of workers based on the fraction of idle workers available</li>
                </ul>
              </li>
              <li>Each worker polls the controller for work to be done</li>
              <li>Workers get a lease on the work and can request more time</li>
              <li>Controller will only accept work from the last worker it assigned the
work to (the tasks are made idempotent)</li>
              <li>GC is done to remove abandoned work results</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Query Subsystem</strong>
        <ul>
          <li>Limited query language
            <ul>
              <li>Includes conditional filtering and group-by</li>
              <li>The authors say that other engines use these primitives to build a
richer query language</li>
            </ul>
          </li>
          <li>Each query is labeled
            <ul>
              <li>Queries that drive dashboards are labeled as having low-latency
requirements</li>
              <li>Bulk query workloads are labeled differently</li>
            </ul>
          </li>
          <li>Query servers for an instance of Mesa are grouped into sets
            <ul>
              <li>Each set can serve all tables</li>
              <li>Similar to <a href="/notes/tao/">TAO</a>, <a href="/notes/unicorn/">Unicorn</a>, and
<a href="/notes/pnuts/">PNUTS</a></li>
              <li>They try to send queries for the same data to the same set for optimal
caching</li>
              <li>Each query server registers the data it actively caches with a
locater service
                <ul>
                  <li>I wonder how the servers determine which data to cache</li>
                  <li>If a client can not find a server that is caching a piece of data how
did the authors decide the client should proceed?</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Multi-DC
    <ul>
      <li><strong>Update</strong>
        <ul>
          <li>A committer coordinates consistent updates across the instances of Mesa
            <ul>
              <li>Assigns each batch a version and writes the data into a datastore using
Paxos
                <ul>
                  <li>Again, they didn’t say PaxosDB as in <a href="/notes/photon/">Photon</a></li>
                </ul>
              </li>
              <li>Committer is stateless</li>
            </ul>
          </li>
          <li>The committer keeps checking to see if the update has propagated to a
sufficient number instances
            <ul>
              <li>This includes checking multiple tables influenced by the update</li>
              <li>Once the criteria is met, the committer updates the committed version
number to reflect the update. Queries will now show the results of the
update</li>
            </ul>
          </li>
          <li>Note how there is no need to have locks between readers and writers</li>
        </ul>
      </li>
      <li>They use a peer-to-peer mechanism to load data onto a new instance and in
the case of recovery</li>
    </ul>
  </li>
  <li>Query Optimizations
    <ul>
      <li>Delta Pruning
        <ul>
          <li>Each delta’s metadata includes the key range affected</li>
          <li>If a delta does not cover data required for a query, it will be skipped</li>
          <li>Particularly effective on time series data
            <ul>
              <li>Since data is stored in key order, later updates will cover only a small
portion of the total keyspace. Queries for old data can just check the
base version.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Scan-to-seek
        <ul>
          <li>Even if the predicate is not on a key not at the front of the prefix,
the authors point out that you can still use the index to skip many
records</li>
        </ul>
      </li>
      <li>Resume Key
        <ul>
          <li>Since most clients stream results, and each block includes a resume key,
clients can issue the same query to another server and resume processing
where it left of. This is great for failover</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Worker Parallelization
    <ul>
      <li>MapReduce is used for many operations. However, it is a challenge to ensure
reasonably equal distribution of load across the key space</li>
      <li>When writing a delta, the worker samples a key every so often and records
it’s offset</li>
      <li>These samples can be used to determine a reasonable input split balance</li>
    </ul>
  </li>
  <li>Schema Changes
    <ul>
      <li>Naive Method:
        <ul>
          <li>Create a copy of the table with the new schema at a specific version</li>
          <li>Replay the updates on the copy until it catches up</li>
          <li>Switch the version used for queries atomically
            <ul>
              <li>They note that queries can still run against the old schema for some
time</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Linked Schema Change
        <ul>
          <li>Makes the change visible immediately while translating to the new schema
on the fly</li>
          <li>All new deltas are written with the new version</li>
          <li>It is not always possible. The authors give the example of a schema change
reversing the sort order of a table</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>


</div>




	

	

	
		
		
		
			<div id="related">
			  <h3><em>Other notes about database</em></h2>
			  	<ul class="list-group">
			    
			    	
			       <li class="list-group-item">
              <a href="/notes/bigtable/"><strong>Bigtable: A Distributed Storage System for Structured Data</strong></a>
              [Google, 2006]
             </li>
            
			    
			    	
			       <li class="list-group-item">
              <a href="/notes/cassandra/"><strong>Cassandra: A Decentralized Structured Storage System</strong></a>
              [Facebook, 2009]
             </li>
            
			    
			    	
			       <li class="list-group-item">
              <a href="/notes/dynamo/"><strong>Dynamo: Amazon's Highly Available Key-value Store</strong></a>
              [Amazon, 2007]
             </li>
            
			    
			    	
			       <li class="list-group-item">
              <a href="/notes/f1/"><strong>F1: A Distributed SQL Database That Scales</strong></a>
              [Google, 2013]
             </li>
            
			    
			    	
			    
			    	
			       <li class="list-group-item">
              <a href="/notes/pnuts/"><strong>PNUTS: Yahoo!'s Hosted Data Serving Platform</strong></a>
              [Yahoo!, 2008]
             </li>
            
			    
			    	
			       <li class="list-group-item">
              <a href="/notes/spanner/"><strong>Spanner: Google's Globally-Distributed Database</strong></a>
              [Google, 2012]
             </li>
            
			    
			    	
			       <li class="list-group-item">
              <a href="/notes/tao/"><strong>TAO: Facebook’s Distributed Data Store for the Social Graph</strong></a>
              [Facebook, 2013]
             </li>
            
			    
			    </ul>
			</div>
		
	

	

	

	

	

	

	

	

	




            <hr>

            
          </div>
          <footer>
              <div class="row">
                <div class="col-lg-12">

                  <ul class="list-unstyled">
                    <li class="pull-right"><a href="#top">Back to top</a></li>
                    <li><a href="/">Home</a></li>
                    <li><a href="/articles">Articles</a></li>
                    <li><a href="/projects">Projects</a></li>
                    <li><a href="/notes">Notes</a></li>
                    <li><a href="/travel">Travel</a></li>
                    <li><a href="/resume">Resume</a></li>
                    <li><a href="/contact">Contact</a></li>
                    <li><a href="http://feeds.feedburner.com/StephenHoliday" rel="alternate" type="application/rss+xml">RSS</a></li>
                    <li><a href="http://ca.linkedin.com/in/stephenholiday"><img src="http://s.c.lnkd.licdn.com/scds/common/u/img/webpromo/btn_profile_greytxt_80x15.png" width="80" height="15" border="0" alt="View Stephen Holiday's profile on LinkedIn"></a></li>
                  </ul>
                  <br />
                  <br>
                  <p>&copy; Stephen Holiday - stephen@stephenholiday.com</p>
                </div>
                
              </div>

            </footer>
        </div> <!-- /container -->

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/bootstrap/3.1.1/js/bootstrap.min.js"></script>

        <script type="text/javascript">
          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-17445820-1']);
          _gaq.push(['_setDomainName', '.stephenholiday.com']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();

        </script>
    </body>
</html>